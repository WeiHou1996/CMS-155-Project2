{"cells":[{"cell_type":"markdown","metadata":{"id":"s1b6zO-CSaVG"},"source":["# Mini Project2\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6P-a96GSSaVJ"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import requests"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["url_dict = {\n","     'data.csv': 'https://caltech-cs155.s3.us-east-2.amazonaws.com/sets/miniprojects/project2/data/data.csv',\n","     'movies.csv': 'https://caltech-cs155.s3.us-east-2.amazonaws.com/sets/miniprojects/project2/data/movies.csv',\n","     'train.csv': 'https://caltech-cs155.s3.us-east-2.amazonaws.com/sets/miniprojects/project2/data/train.csv',\n","     'test.csv': 'https://caltech-cs155.s3.us-east-2.amazonaws.com/sets/miniprojects/project2/data/test.csv'\n","}\n","\n","def download_file(file_path):\n","    url = url_dict[file_path]\n","    print('Start downloading...')\n","    with requests.get(url, stream=True) as r:\n","        r.raise_for_status()\n","        with open(file_path, 'wb') as f:\n","            for chunk in r.iter_content(chunk_size=1024 * 1024 * 1024):\n","                f.write(chunk)\n","    print('Complete')\n","\n","download_file('data.csv')\n","download_file('movies.csv')\n","download_file('train.csv')\n","download_file('test.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HrO0jULdSaVK"},"outputs":[],"source":["def grad_U(Ui, Yij, Vj, ai, bj, mu, reg, eta):\n","    \"\"\"\n","    Takes as input Ui (the ith row of U), a training point Yij, the column\n","    vector Vj (jth column of V^T), reg (the regularization parameter lambda),\n","    and eta (the learning rate).\n","\n","    Returns the gradient of the regularized loss function with\n","    respect to Ui multiplied by eta.\n","    \"\"\"\n","    \n","    grad = reg*Ui - Vj * ((Yij-mu) - np.dot(Ui,Vj) + ai + bj)\n","\n","    foo = eta*grad\n","\n","    return foo\n","        \n","\n","def grad_V(Ui, Yij, Vj, ai, bj, mu, reg, eta):\n","    \"\"\"\n","    Takes as input the column vector Vj (jth column of V^T), a training point Yij,\n","    Ui (the ith row of U), reg (the regularization parameter lambda),\n","    and eta (the learning rate).\n","\n","    Returns the gradient of the regularized loss function with\n","    respect to Vj multiplied by eta.\n","    \"\"\"\n","\n","    grad = reg*Vj - Ui * ((Yij-mu) - np.dot(Ui,Vj) + ai + bj)\n","\n","    foo = eta*grad\n","\n","    return foo\n","\n","def grad_a(Ui, Yij, Vj, ai, bj, mu, reg, eta):\n","    \"\"\"\n","    Takes as input the column vector Vj (jth column of V^T), a training point Yij,\n","    Ui (the ith row of U), reg (the regularization parameter lambda),\n","    and eta (the learning rate).\n","\n","    Returns the gradient of the regularized loss function with\n","    respect to Vj multiplied by eta.\n","    \"\"\"\n","\n","    grad = reg*ai - ((Yij-mu) - (np.dot(Ui,Vj) + ai + bj))\n","\n","    foo = eta*grad\n","\n","    return foo\n","\n","def grad_b(Ui, Yij, Vj, ai, bj, mu, reg, eta):\n","    \"\"\"\n","    Takes as input the column vector Vj (jth column of V^T), a training point Yij,\n","    Ui (the ith row of U), reg (the regularization parameter lambda),\n","    and eta (the learning rate).\n","\n","    Returns the gradient of the regularized loss function with\n","    respect to Vj multiplied by eta.\n","    \"\"\"\n","\n","    grad = reg*bj - ((Yij - mu) - (np.dot(Ui,Vj) + ai + bj))\n","\n","    foo = eta*grad\n","\n","    return foo\n","\n","def get_err(U, V, Y, a, b, mu, reg=0.0, reg_bias=0.0):\n","    \"\"\"\n","    Takes as input a matrix Y of triples (i, j, Y_ij) where i is the index of a user,\n","    j is the index of a movie, and Y_ij is user i's rating of movie j and\n","    user/movie matrices U and V.\n","\n","    Returns the mean regularized squared-error of predictions made by\n","    estimating Y_{ij} as the dot product of the ith row of U and the jth column of V^T.\n","    \"\"\"\n","    N = Y.shape[0]\n","    err = 0\n","    for i in range(N):\n","        Y_idx = Y[i]\n","        Ui  = U[Y_idx[0]-1,:]\n","        Vj  = V[Y_idx[1]-1,:]\n","        Yij = Y_idx[2]\n","        ai  = a[Y_idx[0]-1]\n","        bj  = b[Y_idx[1]-1]\n","\n","        err += ((Yij-mu) - np.dot(Ui,Vj) + ai + bj)**2\n","\n","    err = err/(2*N)\n","    err = err + (reg/2)*(np.linalg.norm(U, ord='fro')**2 + np.linalg.norm(V, ord='fro')**2) + (reg_bias/2)*(np.linalg.norm(a)**2 + np.linalg.norm(b)**2)\n","\n","    return err\n","\n","\n","def train_model(M, N, K, eta, reg, reg_bias, Y, mu, eps=0.0001, max_epochs=300):\n","    \"\"\"\n","    Given a training data matrix Y containing rows (i, j, Y_ij)\n","    where Y_ij is user i's rating on movie j, learns an\n","    M x K matrix U and N x K matrix V such that rating Y_ij is approximated\n","    by (UV^T)_ij.\n","\n","    Uses a learning rate of <eta> and regularization of <reg>. Stops after\n","    <max_epochs> epochs, or once the magnitude of the decrease in regularized\n","    MSE between epochs is smaller than a fraction <eps> of the decrease in\n","    MSE after the first epoch.\n","\n","    Returns a tuple (U, V, err) consisting of U, V, and the unregularized MSE\n","    of the model.\n","    \"\"\"\n","\n","    U = np.random.uniform(-0.5,0.5,size=(M,K)) \n","    V = np.random.uniform(-0.5,0.5,size=(N,K))\n","    a = np.random.uniform(-0.05,0.05,size=(M,1))\n","    b = np.random.uniform(-0.05,0.05,size=(N,1))\n","\n","    err_0 = get_err(U, V, Y, a, b, mu, reg=reg, reg_bias=reg_bias) \n","\n","    N_y = Y.shape[0]\n","\n","    for c in range(max_epochs):\n","        j = np.random.permutation(np.array(range(N_y)))\n","        for i in j:\n","            Y_idx  = Y[i]\n","            Ui  = U[Y_idx[0]-1,:]\n","            Vj  = V[Y_idx[1]-1,:]\n","            Yij = Y_idx[2]\n","            ai  = a[Y_idx[0]-1]\n","            bj  = b[Y_idx[1]-1]\n","\n","            Ui  = Ui - grad_U(Ui, Yij, Vj, ai, bj, mu, reg, eta)\n","            Vj  = Vj - grad_V(Ui, Yij, Vj, ai, bj, mu, reg, eta)\n","            ai  = ai - grad_a(Ui, Yij, Vj, ai, bj, mu, reg_bias, eta)\n","            bj  = bj - grad_b(Ui, Yij, Vj, ai, bj, mu, reg_bias, eta)\n","\n","            U[Y_idx[0]-1,:] = Ui\n","            V[Y_idx[1]-1,:] = Vj\n","            a[Y_idx[0]-1] = ai\n","            b[Y_idx[1]-1] = bj\n","\n","        err = get_err(U, V, Y, a, b, mu, reg=reg, reg_bias=reg_bias)\n","\n","        if c == 0:\n","            err_int = np.abs(err-err_0)\n","            err_1 = 0\n","        \n","        err_ratio = (err_1-err)/err_int\n","\n","        if (err_ratio < eps) and (err_ratio > 0):\n","            err = get_err(U, V, Y, a, b, mu)\n","            break\n","        \n","        err_1 = err\n","\n","    return U, V, a, b, err"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = np.array(pd.read_csv('data.csv'))\n","movies = np.array(pd.read_csv('movies.csv'))\n","train = np.array(pd.read_csv('train.csv'))\n","test = np.array(pd.read_csv('test.csv'))\n","\n","N = train.shape[0]\n","mu = 0\n","for i in range(N):\n","    Y_idx = train[i]\n","    Yij   = Y_idx[2]\n","    mu += Yij\n","mu_train = mu/N"]},{"cell_type":"markdown","metadata":{},"source":["## Case for visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SmexnMN5SaVL"},"outputs":[],"source":["M = max(max(train[:,0]), max(test[:,0])).astype(int) # users\n","N = max(max(train[:,1]), max(test[:,1])).astype(int) # movies\n","print(\"Factorizing with \", M, \" users, \", N, \" movies.\")\n","K = 20\n","\n","reg = 0.1\n","eta = 0.05 # learning rate\n","\n","reg_bias = reg\n","U,V,a,b,err = train_model(M, N, K, eta, reg, reg_bias, train, mu_train, eps=0.005)"]},{"cell_type":"markdown","metadata":{},"source":["## Case Study"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["col = ['black','blue','red','gray','green']\n","M = max(max(train[:,0]), max(test[:,0])).astype(int) # users\n","N = max(max(train[:,1]), max(test[:,1])).astype(int) # movies\n","print(\"Factorizing with \", M, \" users, \", N, \" movies.\")\n","K = 20\n","\n","regs = np.logspace(-2,0,10)\n","etas = [0.01, 0.03, 0.05, 0.07, 0.09] # learning rate\n","fig, ax = plt.subplots()\n","# Use to compute Ein and Eout\n","for i,eta in enumerate(etas):\n","    eta = round(eta,2)\n","    E_in = []\n","    E_out = []\n","    for reg in regs:\n","        print(\"Training model with M = %s, N = %s, k = %s, eta = %s, reg = %s\"%(M, N, K, eta, reg))\n","        reg_bias = 0\n","        U,V,a,b,err = train_model(M, N, K, eta, reg, reg_bias, train, mu_train, eps=0.005)\n","        E_in.append(err)\n","        E_out.append(get_err(U, V, test, a, b, mu_train))\n","\n","    plt.semilogx(regs, E_in, color=col[i], label=f'$\\eta = {eta}$')\n","    plt.semilogx(regs, E_out, linestyle='--')\n","    plt.title('Error vs. $\\lambda$')\n","    plt.ylim([0.1, 0.75])\n","    plt.xlabel(r'$\\lambda$')\n","    plt.ylabel('Error')\n","    plt.legend()\n","plt.savefig('set5_test_bias.png')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
